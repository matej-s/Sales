REEADME
This project contains scripts for the capstone CYO project of HarvardX Data Science Professional Certificate program.
The aim of the project is to develop a system that predicts the success of sales by using data set
which represents sales data,

Data set (IBM Watson Sales-Win-Loss) was originaly obtained on the following GitHub (https://www.kaggle.com/kerneler/starter-ibm-watson-sales-win-loss-5ce297ae-2). These project use these data set (WA_Fn-UseC_-Sales-Win-Loss.csv.zip) from the GitHub repository of this project on following link (https://github.com/matej-s/Sales/blob/main/dataset/WA_Fn-UseC_-Sales-Win-Loss.csv.zip)  and the data from this location is downloaded into the program.

Sales.R - R code used to evaluate data set and build machine learning models.
Sales_report.Rmd - R Markdown script used to create the PDF report.
Sales_report.pdf - pdf report document, result of processing Rmd script.

Project related documentation (Sales.R, Sales_report.Rmd, Sales_report.pdf) can be accessed on the GitHub link https://github.com/matej-s/Sales . 

The project used a Windows 10 computer with an i3 processor and 8 GB of RAM. The program code is written in R (version 4.0.2) and RStudio (version 1.3.1073) was used for development. For the described system, generating a pdf document (Sales_report.pdf) using the Sales_report.Rmd script takes about 7.5 hours, with the most time-consuming, about 90%, falling on the parts related to adaboost algorithm (takes about 4H for chapter 3.7 AdaBoost) and random forest algorithm (takes about 1H for chapter 3.6 Random Forest and 1H for chapter 3.10 Final Validation). 


Table of Contents
1 Introduction  

2 Method and Analysis

2.1 Download Data and Generate Data Sets
2.1.1 Initial Exploration
2.1.2 Project Data Sets

2.2 Exploration and Vizualization
2.2.1 Correlation
2.2.2 Near Zero Variance Predictors
2.2.3 Statistics Summary
2.2.4 QQ Plot
2.2.5 Density Plot
2.2.6 Prevalence
2.2.7 Class Distribution
2.2.8 Feature Selections

2.3 Modeling Approach


3 Results
3.1 Baseline Model
3.2 Generalized Linear Model
3.3 Naive Bayes Model
3.4 K-Nearest Neighbors
3.5 Quadratic Discriminant Analysis
3.6 Random Forest
3.7 AdaBoost
3.8 Ensemble Model
3.9 Best Model Selection
3.10 Final Validation
 
4 Conclusion

5 Appendix